http://cygx.mydns.jp/blog/?arti=559

リスト
1. IMU/カメラ画像/Lidar(Depth)
2. オンライン（リアルタイム）単眼カメラからの密な3次元復元（Dense 3D Reconstruction from monocular image, real-time SfM, Monocular SLAM）

1.
VIRAL (visual-inertial-ranging-lidar) SLAM, sensor fusion
LIC-Fusion: LiDAR-Inertial-Camera Odometry
“Laser–visual–inertial odometry and mapping
with high robustness and low drift,” (J. Zhang and S. Singh)


2.Monocular SLAM
vslam歴史的な流れ(簡易版)->https://noshumi.blogspot.com/2017/05/visual-slam-1visual-slam.html

・DTAM(Dense Tracking and Motion).2011
約10年前 実装がない(OpenDTAMはGithub上にある)　精度良
loop close検出持たない->机の上程度のみ，広範囲のSLAMは不可能
PTAM(2007)は特徴点を抽出する方法(indirect)

動画
https://www.youtube.com/watch?v=Df9WhgibCQA&ab_channel=imperialrobotvision
paper
https://www.doc.ic.ac.uk/~ajd/Publications/newcombe_etal_iccv2011.pdf
GPU上で動く
https://noshumi.blogspot.com/2017/08/lsd-slam.html


・LSD-SLAM(Large Scale).2014

デモ動画でARの車走らせてるやつ
CPUでも動く，directな手法，denseではなくsemi-dense(画像勾配が大きい画素のみ3D復元)
DTAM(2011)と異なりループ検出あるので，机の上だけじゃなくlarge scaleに適用可能
公式ページ->https://vision.in.tum.de/research/vslam/lsdslam

- 解説url
https://noshumi.blogspot.com/2017/08/lsd-slam.html#more
https://www.slideshare.net/kazuya_tennis/slam3-lsdslam
https://www2.slideshare.net/satoshibfujimoto/2cv-lsdslam?next_slideshow=1
https://daily-tech.hatenablog.com/entry/2016/04/29/145737
単眼カメラによるVisual SLAMの原理と3次元再構成の実装例，小林祐一
- LSD-SLAMの関連論文
-- 深度推定
Semi-dense visual odometry for a monocular
camera.2013
SVO(Fast Semi-Direct Monocular Visual Odometry).2014
-- グラフ最適化・Loop Closure
Scale Drift-Aware Large Scale Monocular SLAM.2010

・Dynamic Fusion.2015
動いている人もリアルタイム復元
http://www.youtube.com/watch?v=i1eZekcc_lM


・DSO(Direct Sparse Odometry).2017
https://www.slideshare.net/MasayaKaneko/direct-sparse-odometry


・Yさん実装
https://www.youtube.com/watch?v=TZ1eToXQwN0&ab_channel=MobileRoboticsResearchTeam-MR2T-MobileRoboticsResearchTeam-MR2T-

・課題
課題点→高いフレームレートを要する(>30fps)，復元精度の改善,
正則化（外れ値・ノイズ（人の手が入るなど）除去）特徴量の対応付けに失敗、スケール不定(scale uncertainty)->単眼カメラによるVisual SLAMの原理と3次元再構成の実装例
静止物体ではなく動物体に対応可
テクスチャのない(textureless)部分はどう特徴量える？

cube-slam 深層学習（3d bbox）を上手く取り入れた手法


・用語（https://noshumi.blogspot.com/2017/08/lsd-slam.html）
自己位置推定(tracking)-> 6d camera motion(pose)
パッチ->４pixel*4pixelの四角形
SO(3)->special orthogonal 特殊直行群(slambook-en.pdf(https://github.com/gaoxiang12/slambook-en) page50 式(3.7)を満たすグループの行列のこと)，3次元回転
SE(3)->特殊ユークリッド群，3次元剛体変換(3次元回転+3次元並進の6次元ベクトル)
a×b = a^b，a^は歪対称行列（定義：A^t(Aの転置)=-A）, ベクトルの外積を行列で表したい．^([]x)はオペレータ(slambook-en.pdf page48 式(3.3)より)
ノルム->距離の最小化（https://library.naist.jp/mylimedio/dllimedio/showpdf2.cgi/DLPDFR008313_P1-60）
測光誤差，逆投影関数Π^-1(2D->3D)->
14pageめ/58(https://www.slideshare.net/MasayaKaneko/direct-sparse-odometry)


https://daily-tech.hatenablog.com/entry/2016/04/29/145737
現在のキーフレームを Ki = (Ii,Di,Vi) とすると，新しいイメージ Ij の相対 3次元ポーズ ξji∈se(3) は，下記の分散値で正規化された測光誤差関数を最小化することによって求められる
（カメラの相対姿勢ξを求めるトラッキングは，測光誤差関数を最小化することによって求められる）

inverse depth map -> propagate from frame to frame, refine
image alignment

convergence radius for sim(3) tracking 
coarse to fine approach - pyramid level